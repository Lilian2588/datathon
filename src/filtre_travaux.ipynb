{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7ee7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, udf, concat_ws, to_date, lit\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from shapely.geometry import shape, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dd73730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialisation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datathon_Lyon_Final_All_Points\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583fa4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travaux actifs retenus : 91\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ETAPE 1 : CHARGEMENT ET FILTRAGE TEMPOREL DES TRAVAUX\n",
    "# ---------------------------------------------------------\n",
    "DATE_DEBUT_COLLECTE = \"2025-02-28\"\n",
    "DATE_FIN_COLLECTE = \"2025-06-02\"\n",
    "\n",
    "# Chargement du GeoJSON des travaux\n",
    "df_travaux_raw = spark.read.option(\"multiLine\", True).json(\"../data_travaux/metropole-de-lyon_lyv_lyvia.lyvchantier.json\")\n",
    "\n",
    "# Aplatissement et sélection\n",
    "df_travaux = df_travaux_raw.select(explode(\"features\").alias(\"feature\")) \\\n",
    "    .select(\n",
    "        col(\"feature.properties.gid\").alias(\"work_id\"),\n",
    "        col(\"feature.properties.nature_chantier\").alias(\"nom_chantier\"),\n",
    "        to_date(col(\"feature.properties.date_debut\")).alias(\"date_debut\"),\n",
    "        to_date(col(\"feature.properties.date_fin\")).alias(\"date_fin\"),\n",
    "        col(\"feature.geometry\").alias(\"geometry_struct\")\n",
    "    )\n",
    "\n",
    "# Filtre Temporel : Travaux actifs pendant la collecte\n",
    "df_travaux_filtered = df_travaux.filter(\n",
    "    (col(\"date_debut\") <= lit(DATE_FIN_COLLECTE)) &\n",
    "    (col(\"date_fin\") >= lit(DATE_DEBUT_COLLECTE))\n",
    ")\n",
    "print(f\"Travaux actifs retenus : {df_travaux_filtered.count()}\")\n",
    "\n",
    "# Préparation du Broadcast (Géométries)\n",
    "travaux_list = df_travaux_filtered.collect()\n",
    "prepared_geometries = []\n",
    "\n",
    "for row in travaux_list:\n",
    "    try:\n",
    "        if row['geometry_struct']:\n",
    "            geo_dict = row['geometry_struct'].asDict(recursive=True)\n",
    "            geom = shape(geo_dict)\n",
    "            # Buffer de ~5m\n",
    "            geom_buffer = geom.buffer(0.00005)\n",
    "            prepared_geometries.append({\n",
    "                \"id\": row['work_id'],\n",
    "                \"geom\": geom_buffer\n",
    "            })\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "bc_travaux = spark.sparkContext.broadcast(prepared_geometries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216a9a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total des incidents chargés : 24256\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ETAPE 2 : CHARGEMENT DES INCIDENTS (SANS FILTRE DESCRIPTION)\n",
    "# ---------------------------------------------------------\n",
    "df_incidents_raw = spark.read.option(\"multiLine\", True).json(\"../data_coord/points-rouges-200046977.geojson\")\n",
    "\n",
    "df_incidents = df_incidents_raw.select(explode(\"features\").alias(\"feature\")) \\\n",
    "    .select(\n",
    "        col(\"feature.properties.description\").alias(\"description\"),\n",
    "        col(\"feature.properties.commune\").alias(\"commune\"),\n",
    "        col(\"feature.geometry.coordinates\").alias(\"coords\")\n",
    "    )\n",
    "\n",
    "# --> ICI : AUCUN FILTRE SUR LA DESCRIPTION <--\n",
    "print(f\"Total des incidents chargés : {df_incidents.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950f1d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du croisement spatial...\n",
      "Incidents corrélés avec des travaux : 1759\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ETAPE 3 : CROISEMENT SPATIAL\n",
    "# ---------------------------------------------------------\n",
    "def check_inclusion_optim(coords):\n",
    "    if not coords or len(coords) < 2:\n",
    "        return None\n",
    "    point = Point(coords[0], coords[1])\n",
    "    matches = []\n",
    "    for work in bc_travaux.value:\n",
    "        if work[\"geom\"].contains(point):\n",
    "            matches.append(work[\"id\"])\n",
    "    return matches if len(matches) > 0 else None\n",
    "\n",
    "join_udf = udf(check_inclusion_optim, ArrayType(StringType()))\n",
    "\n",
    "print(\"Lancement du croisement spatial...\")\n",
    "df_resultat = df_incidents.withColumn(\"travaux_id_match\", join_udf(col(\"coords\")))\n",
    "\n",
    "# On ne garde que les incidents liés à un chantier\n",
    "df_final = df_resultat.filter(col(\"travaux_id_match\").isNotNull())\n",
    "print(f\"Incidents corrélés avec des travaux : {df_final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67026ed31e6fd6b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T14:23:05.508151Z",
     "start_time": "2025-12-19T14:22:58.475554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde en cours...\n",
      " Sauvegarde terminée dans 'resultats_croisement_complet/travaux'\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ETAPE 4 : EXPORT CSV\n",
    "# ---------------------------------------------------------\n",
    "df_export = df_final.withColumn(\"travaux_id_match\", concat_ws(\"|\", col(\"travaux_id_match\"))) \\\n",
    "                    .withColumn(\"longitude\", col(\"coords\")[0]) \\\n",
    "                    .withColumn(\"latitude\", col(\"coords\")[1]) \\\n",
    "                    .drop(\"coords\")\n",
    "\n",
    "print(\"Sauvegarde en cours...\")\n",
    "df_export.coalesce(1) \\\n",
    "    .write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"encoding\", \"UTF-8\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"../resultats_croisement_complet/travaux\")\n",
    "\n",
    "print(\" Sauvegarde terminée dans 'resultats_croisement_complet/travaux'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
